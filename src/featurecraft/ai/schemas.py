"""Data schemas and types for AI-powered feature engineering."""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Literal

import pandas as pd


# ============================================================================
# Feature Specifications
# ============================================================================

@dataclass
class FeatureSpec:
    """Specification for a single engineered feature.
    
    This represents a single feature transformation that can be executed
    to generate a new feature from source data.
    
    Attributes:
        name: Feature name (must be unique)
        type: Transformation type (rolling_mean, lag, te, ohe, etc.)
        source_col: Source column name(s) 
        window: Time window for rolling aggregations (e.g., "30d", "7d")
        key_col: Groupby key column (e.g., customer_id)
        time_col: Time column for time-aware operations
        params: Additional transformation parameters
        rationale: Human-readable explanation of feature intent
        safety_tags: Safety validation tags (e.g., ["no_target_ref", "time_safe"])
        priority: Priority for feature selection (1=highest)
        
    Example:
        >>> spec = FeatureSpec(
        ...     name="amt_mean_30d",
        ...     type="rolling_mean",
        ...     source_col="amount",
        ...     window="30d",
        ...     key_col="customer_id",
        ...     time_col="transaction_date",
        ...     rationale="Average spending over last 30 days"
        ... )
    """
    
    name: str
    type: Literal[
        # Aggregations
        "rolling_mean", "rolling_sum", "rolling_std", "rolling_min", "rolling_max",
        "lag", "diff", "pct_change", "ewm", "expanding_mean",
        # Cardinality
        "nunique", "count",
        # Encodings
        "target_encode", "frequency_encode", "count_encode", "ohe", "label_encode",
        "hash_encode", "woe_encode",
        # Binning & Discretization
        "quantile_bin", "custom_bin", "kmeans_bin",
        # Interactions
        "multiply", "divide", "add", "subtract", "ratio",
        # Domain-specific
        "recency", "frequency", "monetary", "rfm_score",
        # Text
        "tfidf", "bow", "text_length", "word_count",
    ]
    source_col: str | list[str]
    
    # Optional time-aware parameters
    window: str | None = None
    key_col: str | None = None
    time_col: str | None = None
    
    # Additional parameters
    params: dict[str, Any] = field(default_factory=dict)
    
    # Metadata
    rationale: str = ""
    safety_tags: list[str] = field(default_factory=list)
    priority: int = 1
    
    def __post_init__(self):
        """Validate feature spec after initialization."""
        if not self.name:
            raise ValueError("Feature name cannot be empty")
        if isinstance(self.source_col, list) and len(self.source_col) == 0:
            raise ValueError("source_col list cannot be empty")


@dataclass
class FeaturePlan:
    """Complete feature engineering plan generated by LLM.
    
    This represents a validated, executable plan for feature engineering
    that can be applied to a dataset.
    
    Attributes:
        version: Plan schema version (for backwards compatibility)
        dataset_id: Unique dataset identifier
        task: ML task type (classification/regression)
        estimator_family: Target estimator family (tree, linear, etc.)
        constraints: User-specified constraints (time_aware, leakage_blocklist, etc.)
        budget: Execution budget (time, features, tokens)
        candidates: List of feature specifications
        rationale: Overall plan rationale
        safety_summary: Validation results
        metadata: Additional metadata (timestamps, LLM info, etc.)
        
    Example:
        >>> plan = FeaturePlan(
        ...     dataset_id="kaggle_telco_churn",
        ...     task="classification",
        ...     estimator_family="tree",
        ...     candidates=[spec1, spec2, spec3],
        ...     rationale="RFM features for churn prediction"
        ... )
    """
    
    # Core attributes
    version: str = "1.0"
    dataset_id: str = ""
    task: Literal["classification", "regression"] = "classification"
    estimator_family: str = "tree"
    
    # Configuration
    constraints: dict[str, Any] = field(default_factory=dict)
    budget: dict[str, Any] = field(default_factory=dict)
    
    # Features
    candidates: list[FeatureSpec] = field(default_factory=list)
    
    # Metadata
    rationale: str = ""
    safety_summary: dict[str, Any] = field(default_factory=dict)
    metadata: dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Set default metadata."""
        if "created_at" not in self.metadata:
            self.metadata["created_at"] = datetime.utcnow().isoformat()
        if "n_features" not in self.metadata:
            self.metadata["n_features"] = len(self.candidates)
    
    def to_dict(self) -> dict[str, Any]:
        """Convert plan to dictionary for serialization."""
        return {
            "version": self.version,
            "dataset_id": self.dataset_id,
            "task": self.task,
            "estimator_family": self.estimator_family,
            "constraints": self.constraints,
            "budget": self.budget,
            "candidates": [
                {
                    "name": c.name,
                    "type": c.type,
                    "source_col": c.source_col,
                    "window": c.window,
                    "key_col": c.key_col,
                    "time_col": c.time_col,
                    "params": c.params,
                    "rationale": c.rationale,
                    "safety_tags": c.safety_tags,
                    "priority": c.priority,
                }
                for c in self.candidates
            ],
            "rationale": self.rationale,
            "safety_summary": self.safety_summary,
            "metadata": self.metadata,
        }
    
    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> FeaturePlan:
        """Create plan from dictionary."""
        candidates = [
            FeatureSpec(**c) for c in data.get("candidates", [])
        ]
        return cls(
            version=data.get("version", "1.0"),
            dataset_id=data.get("dataset_id", ""),
            task=data.get("task", "classification"),
            estimator_family=data.get("estimator_family", "tree"),
            constraints=data.get("constraints", {}),
            budget=data.get("budget", {}),
            candidates=candidates,
            rationale=data.get("rationale", ""),
            safety_summary=data.get("safety_summary", {}),
            metadata=data.get("metadata", {}),
        )
    
    def get_feature_names(self) -> list[str]:
        """Get list of feature names in plan."""
        return [c.name for c in self.candidates]
    
    def filter_by_priority(self, min_priority: int = 1) -> FeaturePlan:
        """Create new plan with only high-priority features."""
        filtered_candidates = [c for c in self.candidates if c.priority >= min_priority]
        return FeaturePlan(
            version=self.version,
            dataset_id=self.dataset_id,
            task=self.task,
            estimator_family=self.estimator_family,
            constraints=self.constraints,
            budget=self.budget,
            candidates=filtered_candidates,
            rationale=self.rationale,
            safety_summary=self.safety_summary,
            metadata=self.metadata,
        )


# ============================================================================
# AI Budget & Constraints
# ============================================================================

@dataclass
class AIBudget:
    """Budget constraints for AI-powered feature planning.
    
    Attributes:
        max_tokens: Maximum LLM tokens per request
        max_requests: Maximum LLM requests per session
        timeout_seconds: Request timeout
        cache_ttl_hours: Cache TTL for plan results
        deterministic_seed: Seed for reproducible LLM outputs
        max_features: Maximum number of features to generate
        max_time_seconds: Maximum execution time for feature generation
        
    Example:
        >>> budget = AIBudget(
        ...     max_tokens=50000,
        ...     max_features=100,
        ...     max_time_seconds=300
        ... )
    """
    
    max_tokens: int = 50000
    max_requests: int = 10
    timeout_seconds: int = 60
    cache_ttl_hours: int = 24
    deterministic_seed: int | None = 42
    max_features: int = 200
    max_time_seconds: int = 300


# ============================================================================
# Telemetry & Observability
# ============================================================================

@dataclass
class AICallMetadata:
    """Metadata for LLM API call tracking and telemetry.
    
    Attributes:
        timestamp: ISO timestamp of call
        prompt_hash: SHA256 hash of prompt (no PII)
        response_hash: SHA256 hash of response
        tokens_used: Number of tokens consumed
        latency_ms: Call latency in milliseconds
        provider: LLM provider name (openai, anthropic, local)
        model: Model name/version
        validator_status: Validation result (pass/fail)
        validator_errors: List of validation errors
        eval_results: Optional evaluation metrics
        cache_hit: Whether result was cached
        cost_usd: Estimated cost in USD
        
    Example:
        >>> metadata = AICallMetadata(
        ...     timestamp="2025-10-03T12:00:00",
        ...     tokens_used=5000,
        ...     latency_ms=1200,
        ...     validator_status="pass"
        ... )
    """
    
    timestamp: str
    prompt_hash: str = ""
    response_hash: str = ""
    tokens_used: int = 0
    latency_ms: int = 0
    provider: str = "openai"
    model: str = "gpt-4o"
    validator_status: Literal["pass", "fail", "skipped"] = "skipped"
    validator_errors: list[str] = field(default_factory=list)
    eval_results: dict[str, float] | None = None
    cache_hit: bool = False
    cost_usd: float = 0.0
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for logging."""
        return {
            "timestamp": self.timestamp,
            "prompt_hash": self.prompt_hash,
            "response_hash": self.response_hash,
            "tokens_used": self.tokens_used,
            "latency_ms": self.latency_ms,
            "provider": self.provider,
            "model": self.model,
            "validator_status": self.validator_status,
            "validator_errors": self.validator_errors,
            "eval_results": self.eval_results,
            "cache_hit": self.cache_hit,
            "cost_usd": self.cost_usd,
        }


# ============================================================================
# Validation Results
# ============================================================================

@dataclass
class ValidationResult:
    """Result of plan validation.
    
    Attributes:
        is_valid: Whether plan passed all checks
        errors: List of error messages
        warnings: List of warning messages
        checks_passed: Dict of check name → passed status
        metadata: Additional validation metadata
        
    Example:
        >>> result = ValidationResult(
        ...     is_valid=False,
        ...     errors=["Leakage detected: churn_date references target"],
        ...     warnings=["High cardinality feature may be expensive"],
        ...     checks_passed={"leakage": False, "schema": True}
        ... )
    """
    
    is_valid: bool
    errors: list[str] = field(default_factory=list)
    warnings: list[str] = field(default_factory=list)
    checks_passed: dict[str, bool] = field(default_factory=dict)
    metadata: dict[str, Any] = field(default_factory=dict)
    
    def __bool__(self) -> bool:
        """Allow using ValidationResult in boolean context."""
        return self.is_valid
    
    def summary(self) -> str:
        """Get human-readable summary."""
        if self.is_valid:
            return f"✓ Validation passed ({len(self.checks_passed)} checks)"
        else:
            return f"✗ Validation failed: {len(self.errors)} errors, {len(self.warnings)} warnings"


# ============================================================================
# Dataset Context
# ============================================================================

@dataclass
class DatasetContext:
    """Context about a dataset for LLM planning.
    
    Attributes:
        df: DataFrame (optional, can use schema/stats instead)
        schema: Column name → dtype mapping
        stats: Summary statistics per column
        sample_rows: Sample rows as dict records
        target: Target column name
        task: Task type (auto-detected if None)
        time_col: Time column for time-series
        key_col: Entity key column (customer_id, user_id, etc.)
        
    Example:
        >>> context = DatasetContext(
        ...     df=train_df,
        ...     target="churn",
        ...     time_col="transaction_date",
        ...     key_col="customer_id"
        ... )
    """
    
    df: pd.DataFrame | None = None
    schema: dict[str, str] = field(default_factory=dict)
    stats: dict[str, dict[str, Any]] = field(default_factory=dict)
    sample_rows: list[dict[str, Any]] = field(default_factory=list)
    target: str = ""
    task: Literal["classification", "regression"] | None = None
    time_col: str | None = None
    key_col: str | None = None
    
    def __post_init__(self):
        """Extract schema/stats from DataFrame if provided."""
        if self.df is not None:
            if not self.schema:
                self.schema = {col: str(dtype) for col, dtype in self.df.dtypes.items()}
            if not self.stats:
                self.stats = self._compute_stats(self.df)
            if not self.sample_rows:
                self.sample_rows = self.df.head(5).to_dict(orient="records")
    
    @staticmethod
    def _compute_stats(df: pd.DataFrame) -> dict[str, dict[str, Any]]:
        """Compute summary statistics for DataFrame."""
        stats = {}
        for col in df.columns:
            col_stats = {
                "dtype": str(df[col].dtype),
                "missing_rate": float(df[col].isna().mean()),
                "unique_count": int(df[col].nunique()),
            }
            
            # Numeric stats
            if pd.api.types.is_numeric_dtype(df[col]):
                col_stats.update({
                    "mean": float(df[col].mean()) if not df[col].isna().all() else None,
                    "median": float(df[col].median()) if not df[col].isna().all() else None,
                    "std": float(df[col].std()) if not df[col].isna().all() else None,
                    "min": float(df[col].min()) if not df[col].isna().all() else None,
                    "max": float(df[col].max()) if not df[col].isna().all() else None,
                })
            
            stats[col] = col_stats
        
        return stats

